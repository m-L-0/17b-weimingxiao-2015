{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/captcha/images/29032.jpg' '2841']\n",
      "一位验证码有9844个\n",
      "两位验证码有5908个\n",
      "三位验证码有16183个\n",
      "四位验证码有8065个\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n",
      "\n",
      "Transform start......\n",
      "Transform done!\n"
     ]
    }
   ],
   "source": [
    "'''统计数据 以及生成tfrecord\n",
    "并且按照8：1：1分割tf文件'''\n",
    "import tensorflow as tf\n",
    "\n",
    "# filename_queue = tf.train.string_input_producer('/home/r/captcha/data/captcha/labels/labels.')\n",
    "import os\n",
    "import csv\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "for root, sub_folders, files in os.walk('./data/captcha/images/'):\n",
    "        for name in files:\n",
    "            infile = os.path.join(root, name)\n",
    "            outfile = os.path.join(root, name)\n",
    "            im = Image.open(infile)\n",
    "            (x, y) = im.size  # read image size\n",
    "            x_s = 56  # define standard width\n",
    "            y_s = 40  # calc height based on standard width\n",
    "            out = im.resize((x_s, y_s), Image.ANTIALIAS)  # resize image with high-quality\n",
    "            out = out.convert('1')\n",
    "            out.save(outfile)\n",
    "csvfile = csv.reader(open('/home/r/captcha/data/captcha/labels/labels.csv', 'r'))\n",
    "images = []\n",
    "labels = []\n",
    "for line in csvfile:\n",
    "    a='./'+line[0]\n",
    "    images.append(a)\n",
    "    labels.append(line[1])\n",
    "# print(images[0])\n",
    "# print(labels[0])\n",
    "temp = np.array([images, labels])\n",
    "temp = temp.transpose()\n",
    "np.random.shuffle(temp)\n",
    "print(temp[0])\n",
    "myone = []\n",
    "mytwo = []\n",
    "mythree = []\n",
    "myfour = []\n",
    "\n",
    "def test(temp, myone, mytwo, mythree, myfour):\n",
    "    a=0\n",
    "    b=0\n",
    "    c=0\n",
    "    d=0\n",
    "    for i in range(40000) :\n",
    "        if (int(temp[i,1]) >9 and int(temp[i,1])<100):\n",
    "            a=a+1\n",
    "            mytwo.append(temp[i])\n",
    "        elif (int(temp[i,1])<10) :\n",
    "            b=b+1\n",
    "            myone.append(temp[i])\n",
    "        elif (int(temp[i,1])>99 and int(temp[i,1])<1000):\n",
    "            c=c+1\n",
    "            mythree.append(temp[i])\n",
    "        else:\n",
    "            d=d+1  \n",
    "            myfour.append(temp[i])\n",
    "    print('一位验证码有%d个' % b)\n",
    "    print('两位验证码有%d个' % a)\n",
    "    print('三位验证码有%d个' % c)\n",
    "    print('四位验证码有%d个' % d)\n",
    "def int64_feature(value):\n",
    "    \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    \n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def convert_to_tfrecord(images, labels, save_dir, name):\n",
    "    '''convert all images and labels to one tfrecord file.\n",
    "    Args:\n",
    "        images: list of image directories, string type\n",
    "        labels: list of labels, int type\n",
    "        save_dir: the directory to save tfrecord file, e.g.: '/home/folder1/'\n",
    "        name: the name of tfrecord file, string type, e.g.: 'train'\n",
    "    Return:\n",
    "        no return\n",
    "    Note:\n",
    "        converting needs some time, be patient...\n",
    "    '''\n",
    "    \n",
    "    filename = os.path.join(save_dir, name + '.tfrecords')\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    if np.shape(images)[0] != n_samples:\n",
    "        raise ValueError('Images size %d does not match label size %d.' %(images.shape[0], n_samples))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # wait some time here, transforming need some time based on the size of your data.\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    print('\\nTransform start......')\n",
    "    for i in np.arange(0, n_samples):\n",
    "        try:\n",
    "            image = io.imread(images[i])  # type(image) must be array!\n",
    "            image_raw = image.tostring()\n",
    "            label = int(labels[i])\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                            'label': int64_feature(label),\n",
    "                            'image_raw': bytes_feature(image_raw)}))\n",
    "            writer.write(example.SerializeToString())\n",
    "        except IOError as e:\n",
    "            print('Could not read:', images[i])\n",
    "            print('error: %s' % e)\n",
    "            print('Skip it!\\n')\n",
    "    writer.close()\n",
    "    print('Transform done!')\n",
    "\n",
    "test(temp, myone, mytwo, mythree, myfour)\n",
    "\n",
    "myone = np.array(myone)\n",
    "np.random.shuffle(myone)\n",
    "mytwo = np.array(mytwo)\n",
    "mythree = np.array(mythree)\n",
    "myfour = np.array(myfour)\n",
    "\n",
    "tra_image_list1 = list(myone[:1100, 0])+list(mytwo[:900, 0])+list(mythree[:1100, 0])+list(myfour[:1000, 0])\n",
    "tra_label_list1 = list(myone[:1100, 1])+list(mytwo[:900, 1])+list(mythree[:1100, 1])+list(myfour[:1000, 1])\n",
    "tra_image_list2 = list(myone[1100:2200, 0])+list(mytwo[900:1800, 0])+list(mythree[1100:2200, 0])+list(myfour[1000:2000, 0])\n",
    "tra_label_list2 = list(myone[1100:2200, 1])+list(mytwo[900:1800, 1])+list(mythree[1100:2200, 1])+list(myfour[1000:2000, 1])\n",
    "tra_image_list3 = list(myone[2200:3300, 0])+list(mytwo[1800:2700, 0])+list(mythree[2200:3300, 0])+list(myfour[2000:3000, 0])\n",
    "tra_label_list3 = list(myone[2200:3300, 1])+list(mytwo[1800:2700, 1])+list(mythree[2200:3300, 1])+list(myfour[2000:3000, 1])\n",
    "tra_image_list4 = list(myone[3300:4400, 0])+list(mytwo[2700:3600, 0])+list(mythree[3300:4400, 0])+list(myfour[3000:4000, 0])\n",
    "tra_label_list4 = list(myone[3300:4400, 1])+list(mytwo[2700:3600, 1])+list(mythree[3300:4400, 1])+list(myfour[3000:4000, 1])\n",
    "tra_image_list5 = list(myone[4400:5500, 0])+list(mytwo[3600:4500, 0])+list(mythree[4400:5500, 0])+list(myfour[4000:5000, 0])\n",
    "tra_label_list5 = list(myone[4400:5500, 1])+list(mytwo[3600:4500, 1])+list(mythree[4400:5500, 1])+list(myfour[4000:5000, 1])\n",
    "tra_image_list6 = list(myone[5500:6600, 0])+list(mytwo[4500:5400, 0])+list(mythree[5500:6600, 0])+list(myfour[5000:6000, 0])\n",
    "tra_label_list6 = list(myone[5500:6600, 1])+list(mytwo[4500:5400, 1])+list(mythree[5500:6600, 1])+list(myfour[5000:6000, 1])\n",
    "val_image_list = list(myone[6600:7700, 0])+list(mytwo[5400:5600, 0])+list(mythree[6600:7700, 0])+list(myfour[6000:7000, 0])\n",
    "val_label_list = list(myone[6600:7700, 1])+list(mytwo[5400:5600, 1])+list(mythree[6600:7700, 1])+list(myfour[6000:7000, 1])\n",
    "test_image_list = list(myone[7700:, 0])+list(mytwo[5600:, 0])+list(mythree[7700:, 0])+list(myfour[7000:, 0])\n",
    "test_label_list = list(myone[7700:, 1])+list(mytwo[5600:, 1])+list(mythree[7700:, 1])+list(myfour[7000:, 1])\n",
    "\n",
    "save_dir = './data'\n",
    "convert_to_tfrecord(tra_image_list1, tra_label_list1, save_dir, 'train1')\n",
    "convert_to_tfrecord(tra_image_list2, tra_label_list2, save_dir, 'train2')\n",
    "convert_to_tfrecord(tra_image_list3, tra_label_list3, save_dir, 'train3')\n",
    "convert_to_tfrecord(tra_image_list4, tra_label_list4, save_dir, 'train4')\n",
    "convert_to_tfrecord(tra_image_list5, tra_label_list5, save_dir, 'train5')\n",
    "convert_to_tfrecord(tra_image_list6, tra_label_list6, save_dir, 'train6')\n",
    "convert_to_tfrecord(val_image_list, val_label_list, save_dir, 'validation')\n",
    "convert_to_tfrecord(test_image_list, test_label_list, save_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "def char2pos(c):\n",
    "    k = ord(c)-ord('0')  \n",
    "    return k\n",
    "def read_and_decode(tfrecords_file, batch_size=25):\n",
    "    '''read and decode tfrecord file, generate (image, label) batches\n",
    "    Args:\n",
    "        tfrecords_file: the directory of tfrecord file\n",
    "        batch_size: number of images in each batch\n",
    "    Returns:\n",
    "        image: 4D tensor - [batch_size, width, height, channel]\n",
    "        label: 1D tensor - [batch_size]\n",
    "    '''\n",
    "    # make an input queue from the tfrecord file\n",
    "    batch_size = batch_size\n",
    "    filename_queue = tf.train.string_input_producer([tfrecords_file])\n",
    "    \n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    img_features = tf.parse_single_example(\n",
    "                                        serialized_example,\n",
    "                                        features={\n",
    "                                               'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                               'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                                               })\n",
    "    image = tf.decode_raw(img_features['image_raw'], tf.uint8)\n",
    "    \n",
    "    ##########################################################\n",
    "    # you can put data augmentation here, I didn't use it\n",
    "    ##########################################################\n",
    "    # all the images of notMNIST are 28*28, you need to change the image size if you use other dataset.\n",
    "    image = tf.reshape(image, [40, 56])\n",
    "    image = tf.cast(image, tf.float32)*(1./255)\n",
    "    label = tf.cast(img_features['label'], tf.int64)    \n",
    "    image_batch, label_batch = tf.train.batch([image, label], batch_size= batch_size, capacity=10000)\n",
    "    with tf.Session() as sess:\n",
    "        i = 0\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        \n",
    "        try:\n",
    "            while not coord.should_stop() and i < 1:\n",
    "                # just plot one batch size \n",
    "                \n",
    "                image, label = sess.run([image_batch, label_batch])\n",
    "                #plot_images(image, label)\n",
    "                # image = image.reshape(784)\n",
    "                i += 1\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done!')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    w, = label.shape\n",
    "    # # print(s)\n",
    "    b = np.empty((w, 44))\n",
    "    for j in range(w):\n",
    "        # print(list(str(label[j])))\n",
    "        for i, c in enumerate(list(str(label[j]))):\n",
    "            idx = i * 10 + char2pos(c)\n",
    "            b[j][idx] = 1\n",
    "        for q in range(44):\n",
    "            if b[j][q] != 1:\n",
    "                b[j][q] = 0\n",
    "        for w in range(4-len(str(label[j]))):\n",
    "            b[j][(w+len(str(label[j]))+1)*11-1] = 1\n",
    "        # print(b[j])\n",
    "    s, d, f = image.shape\n",
    "    images = np.empty((s, d*f))\n",
    "    for q in range(s):\n",
    "        c = image[q]\n",
    "        images[q] = c.reshape(d*f)\n",
    "        # image=images\n",
    "    return images, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e1d59cf54408>:169: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "WARNING:tensorflow:From <ipython-input-1-e1d59cf54408>:171: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "WARNING:tensorflow:From <ipython-input-1-e1d59cf54408>:173: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "WARNING:tensorflow:From <ipython-input-1-e1d59cf54408>:175: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "INFO:tensorflow:Restoring parameters from ./cnn/capcha-7400\n",
      "training1 acc 0.9,training1 acc 1,training1 acc 1,training1 acc 1\n",
      "testing step 100, test_acc_sum 88.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88970024108886714"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import read\n",
    "import random\n",
    "# mnist = input_data.read_data_sets('data/fashion', one_hot=True)\n",
    "def cnn():\n",
    "    # FLAGS = tf.app.flags.FLAGS\n",
    "    # tf.app.flags.DEFINE_string('my_list', './cnn', \"\"\"存放模型的目录\"\"\")\n",
    "\n",
    "    # tf.app.flags.DEFINE_string('my_cnn', 'capcha', \"\"\"模型的名称\"\"\")\n",
    "    [X_test, y_test] = read.read_and_decode('./data/test.tfrecords', 1000)\n",
    "    [X_val, y_val] = read.read_and_decode('./data/validation.tfrecords', 2000)\n",
    "\n",
    "\n",
    "\n",
    "    def chooseone(image, label, batchsize):\n",
    "        q = image.shape[0]\n",
    "        im = np.empty((batchsize, 2240))\n",
    "        la = np.empty((batchsize, 44))\n",
    "        for i in range(batchsize):\n",
    "            a = random.randint(0, q-1)\n",
    "            im[i] = image[a]\n",
    "            la[i] = label[a]\n",
    "        return im, la\n",
    "    # 权值初始化\n",
    "\n",
    "\n",
    "    def weight_variable(shape):\n",
    "        # 用正态分布来初始化权值\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        # 本例中用relu激活函数，所以用一个很小的正偏置较好\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # 定义卷积层\n",
    "\n",
    "\n",
    "    def conv2d(x, W):\n",
    "        # 默认 strides[0]=strides[3]=1, strides[1]为x方向步长，strides[2]为y方向步长\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # pooling 层\n",
    "\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    # 模型文件所在的文件夹，是否存在，如果不存在，则创建文件夹\n",
    "    ckpt = tf.train.latest_checkpoint('./cnn')\n",
    "    # # ckpt = tf.train.latest_checkpoint(my_list)\n",
    "    if not ckpt:\n",
    "        if not os.path.exists('./cnn'):\n",
    "            os.mkdir('./cnn')\n",
    "    X_ = tf.placeholder(tf.float32, [None, 2240])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 44])\n",
    "    y_1 = tf.placeholder(tf.float32, [None, 11])\n",
    "    y_2 = tf.placeholder(tf.float32, [None, 11])\n",
    "    y_3 = tf.placeholder(tf.float32, [None, 11])\n",
    "    y_4 = tf.placeholder(tf.float32, [None, 11])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # 把X转为卷积所需要的形式\n",
    "    with tf.name_scope(\"reshape1\"):\n",
    "        X = tf.reshape(X_, [-1, 40, 56, 1])\n",
    "    # 第一层卷积：3×3×1卷积核32个 [3，3，1，32],h_conv1.shape=[-1, 40, 56, 32],学习32种特征\n",
    "    with tf.name_scope(\"conv1_1\"):\n",
    "        W_conv1 = weight_variable([3, 3, 1, 32])\n",
    "        b_conv1 = bias_variable([32]) \n",
    "        h_conv1 = tf.nn.relu(conv2d(X, W_conv1) + b_conv1)\n",
    "\n",
    "    with tf.name_scope(\"pool1\"):\n",
    "        # 第一个pooling 层[-1, 40, 56, 32]->[-1, 20, 28, 32]\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    with tf.name_scope(\"conv1_2\"):\n",
    "         # 第二层卷积：5×5×32卷积核64个 [3，3，32，48],h_conv2.shape=[-1, 20, 28, 48]\n",
    "        W_conv1_ = weight_variable([3, 3, 32, 48])\n",
    "        b_conv1_ = bias_variable([48]) \n",
    "        h_conv1_ = tf.nn.relu(conv2d(h_pool1, W_conv1_) + b_conv1_)\n",
    "\n",
    "    # 第三层卷积：5×5×32卷积核64个 [3，3，32，64],h_conv2.shape=[-1, 20, 28, 64]\n",
    "    with tf.name_scope(\"conv2_1\"):\n",
    "        W_conv2 = weight_variable([3, 3, 48, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_conv1_, W_conv2) + b_conv2)\n",
    "\n",
    "    with tf.name_scope(\"pool2\"):\n",
    "    # 第二个pooling 层,[-1, 20, 28, 64]->[-1, 10, 14, 64] \n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    with tf.name_scope(\"conv3_1\"):\n",
    "    # 第三层卷积：5×5×32卷积核64个 [3，3，64，96],h_conv2.shape=[-1, 20, 28, 96]\n",
    "        W_conv3 = weight_variable([3, 3, 64, 96])\n",
    "        b_conv3 = bias_variable([96])\n",
    "        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    with tf.name_scope(\"pool3\"):\n",
    "        # 第三个pooling 层,[-1, 10, 14, 96]->[-1, 5, 7, 96] \n",
    "        h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "    # flatten层，[-1, 5, 7, 96]->[-1, 5*7*96],即每个样本得到一个7*7*64维的样本\n",
    "    with tf.name_scope(\"flatting\"):\n",
    "        h_pool2_flat = tf.reshape(h_pool3, [-1, 5*7*96])\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "    # fc1\n",
    "        W_fc1 = weight_variable([5*7*96, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # dropout: 输出的维度和h_fc1一样，只是随机部分值被值为零\n",
    "    # keep_prob = tf.placeholder(tf.float32)\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # 输出层\n",
    "    with tf.name_scope(\"y_cov1\"):\n",
    "        W_fc2 = weight_variable([1024, 11])\n",
    "        b_fc2 = bias_variable([11])\n",
    "        y_conv1 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    with tf.name_scope(\"y_cov2\"):\n",
    "        W_fc3 = weight_variable([1024, 11])\n",
    "        b_fc3 = bias_variable([11])\n",
    "        y_conv2 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc3) + b_fc3)\n",
    "    with tf.name_scope(\"y_cov3\"):\n",
    "        W_fc4 = weight_variable([1024, 11])\n",
    "        b_fc4 = bias_variable([11])\n",
    "        y_conv3 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc4) + b_fc4)\n",
    "    with tf.name_scope(\"y_cov4\"):\n",
    "        W_fc5 = weight_variable([1024, 11])\n",
    "        b_fc5 = bias_variable([11])\n",
    "        y_conv4 = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc5) + b_fc5)\n",
    "    # 1.损失函数：cross_entropy\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        cross_entropy1 = -tf.reduce_sum(y_1 * tf.log(y_conv1), name='cross_entropy_1') \n",
    "        cross_entropy2 = -tf.reduce_sum(y_2 * tf.log(y_conv2), name='cross_entropy_2') \n",
    "        cross_entropy3 = -tf.reduce_sum(y_3 * tf.log(y_conv3), name='cross_entropy_3') \n",
    "        cross_entropy4 = -tf.reduce_sum(y_4 * tf.log(y_conv4), name='cross_entropy_4') \n",
    "        cross_entropy = (cross_entropy1+cross_entropy2+cross_entropy3+cross_entropy4)/4\n",
    "        tf.summary.scalar('loss_1', cross_entropy1)  \n",
    "        tf.summary.scalar('loss_2', cross_entropy2)\n",
    "        tf.summary.scalar('loss_3', cross_entropy3)\n",
    "        tf.summary.scalar('loss_4', cross_entropy4)\n",
    "        # tf.summary.scalar('loss', cross_entropy)\n",
    "        merged = tf.summary.merge_all() \n",
    "    # cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_conv, labels=y_))\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=y_))\n",
    "    # 2.优化函数：AdamOptimizer\n",
    "    with tf.name_scope(\"Adam\"):\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    # 3.预测准确结果统计\n",
    "    # 预测值中最大值（１）即[分类结果，是否等于原始标签中的（１）的位置。argmax()取最大值所在的下标\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        y_conva = tf.concat([y_conv1, y_conv2], 1) \n",
    "        y_convb = tf.concat([y_conv3, y_conv4], 1)\n",
    "        y_conv = tf.concat([y_conva, y_convb], 1)\n",
    "        predict = tf.reshape(y_conv, [-1, 4, 11])\n",
    "        max_idx_p = tf.argmax(predict, 2)\n",
    "        max_idx_l = tf.argmax(tf.reshape(y_, [-1, 4, 11]), 2)\n",
    "        correct_pred = tf.equal(max_idx_p, max_idx_l)\n",
    "        # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.cast(tf.reduce_mean(tf.cast(correct_pred,tf.float32),1),tf.int64),tf.float32))\n",
    "        # accuracy = tf.floor(accuracy\n",
    "        correct_prediction1 = tf.equal(tf.argmax(y_conv1, 1), tf.arg_max(y_1, 1))  # \n",
    "        accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32))\n",
    "        correct_prediction2 = tf.equal(tf.argmax(y_conv2, 1), tf.arg_max(y_2, 1))  \n",
    "        accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
    "        correct_prediction3 = tf.equal(tf.argmax(y_conv3, 1), tf.arg_max(y_3, 1))  \n",
    "        accuracy3 = tf.reduce_mean(tf.cast(correct_prediction3, tf.float32))\n",
    "        correct_prediction4 = tf.equal(tf.argmax(y_conv4, 1), tf.arg_max(y_4, 1))  \n",
    "        accuracy4 = tf.reduce_mean(tf.cast(correct_prediction4, tf.float32))\n",
    "    test_acc_sum = tf.Variable(0.0)\n",
    "    batch_acc = tf.placeholder(tf.float32)\n",
    "    new_test_acc_sum = tf.add(test_acc_sum, batch_acc)\n",
    "    update = tf.assign(test_acc_sum, new_test_acc_sum)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    train_writer = tf.summary.FileWriter('graphs/')\n",
    "    train_writer.add_graph(tf.get_default_graph())\n",
    "    # 定义了变量必须要初始化，或者下面形式\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # writer = tf.summary.FileWriter(FLAGS.my_list, sess.graph)\n",
    "        ckpt = tf.train.latest_checkpoint('./cnn')\n",
    "        step = 0\n",
    "        if ckpt:\n",
    "            saver.restore(sess=sess, save_path=ckpt)\n",
    "            step = int(ckpt[len(os.path.join('./cnn', 'capcha')) + 1:])\n",
    "        \n",
    "   \n",
    "    # 全部训练完了再做测试，batch_size=100\n",
    "        for i in range(100): \n",
    "            X_batch, y_batch = chooseone(X_test, y_test, 100)\n",
    "            # X_batch, y_batch = mnist.test.next_batch(batch_size=100)\n",
    "            test_acc = accuracy.eval(feed_dict={X_: X_batch, y_: y_batch, y_1: y_batch[:, :11], y_2: y_batch[:, 11:22], y_3: y_batch[:, 22:33], y_4: y_batch[:, 33:44], keep_prob: 1.0})\n",
    "            summary, train_accuracy1, train_accuracy2, train_accuracy3, train_accuracy4 = sess.run([merged,accuracy1, accuracy2, accuracy3, accuracy4], feed_dict={X_: X_batch, y_: y_batch, y_1: y_batch[:, :11], y_2: y_batch[:, 11:22], y_3: y_batch[:, 22:33], y_4: y_batch[:, 33:44], keep_prob: 1.0})\n",
    "            update.eval(feed_dict={batch_acc: test_acc})\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('training1 acc %g,training1 acc %g,training1 acc %g,training1 acc %g' % (train_accuracy1, train_accuracy2, train_accuracy3, train_accuracy4))\n",
    "                print(\"testing step %d, test_acc_sum %g\" % (i+1, test_acc_sum.eval()))\n",
    "        d=test_acc_sum.eval() / 100.0\n",
    "    return d\n",
    "\n",
    "cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
